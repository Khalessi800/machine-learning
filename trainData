from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Score sur l'ensemble de test
print(f"Score du modèle (test) : {clf.score(X_test, y_test):.3f}")
# Calcul des prédictions
y_pred = clf.predict(X_test)


def drawRocCurve(classifier, nameClassifier, X_test, y_test):
    ns_probs = [0 for _ in range(len(y_test))]
    lr_probs = classifier.predict_proba(X_test)[:, 1]  # Probabilités pour la classe positive
    # Calculer l'AUC
    ns_auc = roc_auc_score(y_test, ns_probs)
    lr_auc = roc_auc_score(y_test, lr_probs)
    print(f"Modèle aléatoire : ROC AUC={ns_auc:.3f}")
    print(f"{nameClassifier} : ROC AUC={lr_auc:.3f}")
    # Tracer les courbes ROC
    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
    plt.figure(figsize=(6, 4))
    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Modèle aléatoire')
    plt.plot(lr_fpr, lr_tpr, marker='.', label=nameClassifier)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.title('Courbe ROC')
    plt.show()
    return lr_auc
# Tracer la courbe ROC
drawRocCurve(clf, "DecisionTreeClassifier", X_test,y_test)

# Afficher la matrice de confusion avec une taille plus petite
cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize=(6, 4))
ConfusionMatrixDisplay(cm).plot(cmap=plt.cm.Blues, ax=ax)
plt.title('Matrice de Confusion')
plt.show()

# Afficher les métriques de performance
print("Classification Report :")
print(classification_report(y_test, y_pred, target_names=['<=50K', '>50K']))
